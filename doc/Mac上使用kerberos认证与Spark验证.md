# Mac上使用kerberos认证与Spark验证

## 说明

macbook是自带kerberos工具的，所以只要拿到krb5.conf和对应的keytab，就可以访问公司服务器开启了kerberos认证的hdp集群。

## 下载配置

登录hdp集群的服务器上拷贝`/ect/krb5.conf`文件到macbook的`/etc`目录下。

```
[libdefaults]
  renew_lifetime = 7d
  forwardable = true
  default_realm = EXAMPLE.COM
  ticket_lifetime = 24h
  dns_lookup_realm = false
  dns_lookup_kdc = false
  default_ccache_name = /tmp/krb5cc_%{uid}
  #default_tgs_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5
  #default_tkt_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5

[logging]
  default = FILE:/var/log/krb5kdc.log
  admin_server = FILE:/var/log/kadmind.log
  kdc = FILE:/var/log/krb5kdc.log

[realms]
  EXAMPLE.COM = {
    admin_server = 10.93.6.247
    kdc = 10.93.6.247
  }
```

再拷贝hive的keytab到macbook上。

![image-20210805181131484](http://image-picgo.test.upcdn.net/img/20210805181131.png)

## mac上认证hive用户

```
 kinit -kt ~/Downloads/hive.service.keytab hive/hadoop-master
```

![image-20210805181235784](http://image-picgo.test.upcdn.net/img/20210805181235.png)



## mac上运行spark程序访问服务器上的hdfs

### 配置mac上的spark客户端

首先需要配置SPARK_HOME/conf目录下的`core-site.xml`。

```
<?xml version="1.0" encoding="UTF-8"?>

<!--Autogenerated by Cloudera Manager-->
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://10.93.6.247:8020</value>
  </property>

  
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>
</configuration>
```

1. `fs.defaultFS`配置远程服务器上部署的hdfs的访问地址（该hdfs已经开启kerberos认证）

2. `hadoop.security.authentication`配置spark访问hdfs时使用kerberos认证。

   如果不配置，会报下面错误。

   ![image-20210806092721695](http://image-picgo.test.upcdn.net/img/20210806092721.png)



### 启动spark-shell

在mac上认证kerberos用户为**hive**。

```
kinit -kt ~/Downloads/hive.service.keytab hive/hadoop-master
```

使用`klist`验证已经登录成功。

![image-20210806093201723](http://image-picgo.test.upcdn.net/img/20210806093201.png)

启动spark-shell。

```
spark-shell --master local
```

![image-20210806092920385](http://image-picgo.test.upcdn.net/img/20210806092920.png)

可以看到正常读写远程服务器上的hdfs。

登录远程服务器，查看spark在hdfs上写出的文件的所属权，可以看到是hive用户。

![image-20210806093024576](http://image-picgo.test.upcdn.net/img/20210806093024.png)