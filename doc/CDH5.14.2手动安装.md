[TOC]

# CDH5.14.2æ‰‹åŠ¨å®‰è£…

## ä¸‹è½½

| è½¯ä»¶       | ç‰ˆæœ¬                      | ä¸‹è½½åœ°å€                                                     |
| ---------- | ------------------------- | ------------------------------------------------------------ |
| jdk        | jdk-8u172-linux-x64       | [ç‚¹å‡»ä¸‹è½½](http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz) |
| hadoop     | hadoop-2.6.0-cdh5.14.2    | [ç‚¹å‡»ä¸‹è½½](http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.2.tar.gz) |
| zookeeeper | zookeeper-3.4.5-cdh5.14.2 | [ç‚¹å‡»ä¸‹è½½](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2.tar.gz) |
| hbase      | hbase-1.2.0-cdh5.14.2     | [ç‚¹å‡»ä¸‹è½½](http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.14.2.tar.gz) |
| hive       | hive-1.1.0-cdh5.14.2      | [ç‚¹å‡»ä¸‹è½½](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.14.2.tar.gz) |
| hue        | hue-3.9.0-cdh5.14.2       | [ç‚¹å‡»ä¸‹è½½](http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.14.2.tar.gz) |
| presto     | presto-server-0.221       | [ç‚¹å‡»ä¸‹è½½](https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.221/presto-server-0.221.tar.gz) |
| kafka      | kafka_2.12-2.1.1          | [ç‚¹å‡»ä¸‹è½½](http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.1.1/kafka_2.12-2.1.1.tgz) |



æ³¨ï¼š CDH5çš„æ‰€æœ‰è½¯ä»¶å¯ä»¥åœ¨æ­¤ä¸‹è½½ï¼š<http://archive.cloudera.com/cdh5/cdh/5/>



## é¢„å¤‡ç¯å¢ƒ

1. jdk8

2. å…³é—­é˜²ç«å¢™

3. è®¾ç½®hostnameï¼Œè®¾ç½®hostsæ–‡ä»¶

   **è®¾ç½®æˆhadoop0**

4. è®¾ç½®ssh

   Now check that you can ssh to the localhost without a passphrase:

   ```
     $ ssh localhost
   ```

   If you cannot ssh to localhost without a passphrase, execute the following commands:

   ```
     $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
     $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
   ```

   

## Hdfs

### å‚è€ƒå®˜æ–¹æ–‡æ¡£

<http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.2/hadoop-project-dist/hadoop-common/SingleCluster.html>

### é…ç½®

etc/hadoop/core-site.xml:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:8020</value>
    </property>
   <!-- æœ¬æœºé»˜è®¤å­˜å‚¨è·¯å¾„ï¼š/tmp/hadoop-huzekang/dfs/name -->
     <!-- é˜²æ­¢é‡å¯ç”µè„‘ä¸´æ—¶æ–‡ä»¶åˆ é™¤æ¸…æ¥šæ‰€æœ‰hdfsä¿å­˜çš„æ–‡ä»¶,è¯¥ç›®å½•éœ€æ‰‹åŠ¨åˆ›å»º -->
      <property>
	        <name>hadoop.tmp.dir</name>
	        <value>/Users/huzekang/opt/hadoop-cdh/data/hadoop/tmp</value>
	    </property>
</configuration>
```

etc/hadoop/hdfs-site.xml:

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

etc/hadoop/hadoop-env.sh 

```
# The java implementation to use.
export JAVA_HOME=/usr/java/jdk1.8.0_211-amd64/
```

### è¿è¡Œ

The following instructions are to run a **MapReduce job locally**. If you want to execute a job on YARN, see [YARN on Single Node](http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.2/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node).

1. Format the filesystem:

   ```
     $ bin/hdfs namenode -format
   ```

   ![](http://image-picgo.test.upcdn.net/img/20191216001215.png)

   Start NameNode daemon and DataNode daemon:

   ```
     $ sbin/start-dfs.sh
   ```

   The hadoop daemon log output is written to the `$HADOOP_LOG_DIR` directory (defaults to `$HADOOP_HOME/logs`).

   ![](https://raw.githubusercontent.com/huzekang/picbed/master/20190618172745.png)

3. Browse the web interface for the NameNode; by default it is available at:

   - NameNode - `http://localhost:50070/`

     ![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623160757.png)

4. Make the HDFS directories required to execute MapReduce jobs:

   ```
     $ bin/hdfs dfs -mkdir /user
     $ bin/hdfs dfs -mkdir /user/<username>
   ```

5. ä¸Šä¼ README.txtåˆ°hdfsä¸Š:

   ```
     $ bin/hdfs dfs -put README.txt /user/README.txt
   ```

6. æäº¤ä¸€ä¸ªè®¡ç®—ä½œä¸šåˆ°hadoop(æ²¡é…ç½®yarnæ˜¯ä½¿ç”¨localæ¨¡å¼è·‘çš„):

   ```
     $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar grep /user/README.txt /user/output 'dfs[a-z.]+'
   ```

7. å°†è®¡ç®—ç»“æœç›®å½•ä»hdfsä¸‹è½½åˆ°æœ¬åœ°:

   ```
  $ bin/hdfs dfs -get /user/output output
     $ ll output/
   ```
   
   or

   View the output files on the distributed filesystem:

   ```
  bin/hdfs dfs -ls /user/huzekang/output
   ```
   
8. When you're done, stop the daemons with:

   ```
     $ sbin/stop-dfs.sh
   ```

## YARN on Single Node

You can run a MapReduce job on YARN in a **pseudo-distributed mode** by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.

### é…ç½®

The following instructions assume that 1. ~ 4. steps of [the above instructions](http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Execution) are already executed.

1. Configure parameters as follows:

   etc/hadoop/mapred-site.xml:

   **å¦‚æœæ²¡æœ‰åˆ™å¤åˆ¶å®ƒçš„templateåˆ›å»ºä¸€ä¸ª**

   ```xml
   <configuration>
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

   etc/hadoop/yarn-site.xml:

   ```xml
   <configuration>
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
     
   <property>
       <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
     
      <property>
       <name>yarn.resourcemanager.address</name>
       <value>127.0.0.1:8032</value>
     </property>
     <property>
       <name>yarn.resourcemanager.scheduler.address</name>
       <value>127.0.0.1:8030</value>
     </property>
     <property>
       <name>yarn.resourcemanager.resource-tracker.address</name>
       <value>127.0.0.1:8031</value>
     </property>
   
   </configuration>
   ```

### è¿è¡Œ

1. Start ResourceManager daemon and NodeManager daemon:

   ```
     $ sbin/start-yarn.sh
   ```

2. Browse the web interface for the ResourceManager; by default it is available at:

   - ResourceManager - `http://localhost:8088/`

     ![](https://i.loli.net/2019/11/19/K98zDSentWXEOGH.png)
     
     å¯åŠ¨è¦çœ‹åˆ°èŠ‚ç‚¹æ‰æ˜¯æˆåŠŸçš„ã€‚å¦‚æœèŠ‚ç‚¹å¥åº·çŠ¶æ€ä¸ºfalseï¼Œæ£€æŸ¥ä¸€ä¸‹ç¡¬ç›˜å¤Ÿä¸å¤Ÿï¼Œæˆ‘æ¸…ç†äº†macçš„ç¡¬ç›˜ç©ºé—´åå°±æ²¡äº‹äº†ã€‚

3. Run a MapReduce job.

   ```
     $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar grep /user/README.txt /user/output 'dfs[a-z.]+'
   ```

   ![](https://raw.githubusercontent.com/huzekang/picbed/master/20190618175408.png)

   ![](http://image-picgo.test.upcdn.net/img/20191216003421.png)

4. When you're done, stop the daemons with:

   ```
     $ sbin/stop-yarn.sh
   ```



**è®¾ç½®äº†yarnå¯åŠ¨ä½œä¸šä¹‹åå°±ä¸èƒ½å†ä½¿ç”¨localæ¨¡å¼äº†ï¼Œæ‰€ä»¥å¦‚æœå…³é—­yarnå†å¯åŠ¨ä½œä¸šå°±ä¼šå‡ºç°ä¸‹é¢æç¤ºã€‚å¦‚æœæƒ³ç”¨å›localæ¨¡å¼ï¼Œå¯ä»¥æŠŠä¸Šé¢yarnçš„é…ç½®æ³¨é‡Šæ‰å³å¯ã€‚**

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190618180037.png)



## Zookeeper

### å‚è€ƒæ–‡æ¡£

<http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2/zookeeperStarted.html#sc_Prerequisites>



### Standalone Operationå®‰è£…

Setting up a ZooKeeper server in standalone mode is straightforward. The server is contained in a single JAR file, so installation consists of creating a configuration.

Once you've downloaded a stable ZooKeeper release unpack it and cd to the root

To start ZooKeeper you need a configuration file. Here is a sample, create it in **conf/zoo.cfg**:

```
tickTime=2000
dataDir=/Users/huzekang/opt/hadoop-cdh/data/zookeeper
clientPort=2181
```

This file can be called anything, but for the sake of this discussion call it **conf/zoo.cfg**. Change the value of **dataDir** to specify an existing (empty to start with) directory. Here are the meanings for each of the fields:

- **tickTime**

  the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime.

- **dataDir**

  the location to store the in-memory database snapshots and, unless specified otherwise, the transaction log of updates to the database.

- **clientPort**

  the port to listen for client connections

Now that you created the configuration file, you can **start ZooKeeper**:

```
bin/zkServer.sh start
```

å¯åŠ¨çš„æ˜¯ä¸€ä¸ªjavaè¿›ç¨‹ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626145350.png)

ZooKeeper logs messages using log4j -- more detail available in the [Logging](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2/zookeeperProgrammers.html#Logging) section of the Programmer's Guide. You will see log messages coming to the console (default) and/or a log file depending on the log4j configuration.

The steps outlined here run ZooKeeper in standalone mode. There is no replication, so if ZooKeeper process fails, the service will go down. This is fine for most development situations, but to run ZooKeeper in replicated mode, please see [Running Replicated ZooKeeper](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2/zookeeperStarted.html#sc_RunningReplicatedZooKeeper).



### Managing ZooKeeper Storage

For long running production systems ZooKeeper storage must be managed externally (dataDir and logs). See the section on [maintenance](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2/zookeeperAdmin.html#sc_maintenance) for more details.



### Connecting to ZooKeeper

```
$ bin/zkCli.sh -server 127.0.0.1:2181
```

This lets you perform simple, file-like operations.

å¯åŠ¨çš„æ˜¯ä¸€ä¸ªjavaè¿›ç¨‹ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626145423.png)

Once you have connected, you should see something like:

```shell
Connecting to localhost:2181
log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).
log4j:WARN Please initialize the log4j system properly.
Welcome to ZooKeeper!
JLine support is enabled
[zkshell: 0]
        
```

From the shell, type help to get a listing of commands that can be executed from the client, as in:

```shell
[zkshell: 0] help
ZooKeeper host:port cmd args
        get path [watch]
        ls path [watch]
        set path data [version]
        delquota [-n|-b] path
        quit
        printwatches on|off
        createpath data acl
        stat path [watch]
        listquota path
        history
        setAcl path acl
        getAcl path
        sync path
        redo cmdno
        addauth scheme auth
        delete path [version]
        setquota -n|-b val path

        
```

From here, you can try a few simple commands to get a feel for this simple command line interface. First, start by issuing the list command, as in ls, yielding:

```shell
[zkshell: 8] ls /
[zookeeper]
        
```

Next, create a new znode by running create /zk_test my_data. This creates a new znode and associates the string "my_data" with the node. You should see:

```shell
[zkshell: 9] create /zk_test my_data
Created /zk_test
      
```

Issue another ls / command to see what the directory looks like:

```shell
[zkshell: 11] ls /
[zookeeper, zk_test]

        
```

Notice that the zk_test directory has now been created.

Next, verify that the data was associated with the znode by running the get command, as in:

```shell
[zkshell: 12] get /zk_test
my_data
cZxid = 5
ctime = Fri Jun 05 13:57:06 PDT 2009
mZxid = 5
mtime = Fri Jun 05 13:57:06 PDT 2009
pZxid = 5
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0
dataLength = 7
numChildren = 0
        
```

We can change the data associated with zk_test by issuing the set command, as in:

```shell
[zkshell: 14] set /zk_test junk
cZxid = 5
ctime = Fri Jun 05 13:57:06 PDT 2009
mZxid = 6
mtime = Fri Jun 05 14:01:52 PDT 2009
pZxid = 5
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0
dataLength = 4
numChildren = 0
[zkshell: 15] get /zk_test
junk
cZxid = 5
ctime = Fri Jun 05 13:57:06 PDT 2009
mZxid = 6
mtime = Fri Jun 05 14:01:52 PDT 2009
pZxid = 5
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0
dataLength = 4
numChildren = 0
      
```

(Notice we did a get after setting the data and it did, indeed, change.

Finally, let's delete the node by issuing:

```
[zkshell: 16] delete /zk_test
[zkshell: 17] ls /
[zookeeper]
[zkshell: 18]
```

å¦‚æœéœ€è¦é€’å½’åˆ é™¤åˆ™å¯ä»¥ä½¿ç”¨`rmr /hbase`



## Hbase

### é…ç½®confç›®å½•ä¸‹æ–‡ä»¶

#### hbase-env.sh

é…ç½®javaç¯å¢ƒ

```shell
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home
```

æ³¨é‡Šjdk7éœ€è¦çš„é…ç½®ï¼Œå› ä¸ºæˆ‘ä½¿ç”¨çš„æ˜¯jdk8

```shell
#export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"
#export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"
```

è®¾ç½®hbase pidæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºå…³é—­hbaseä½¿ç”¨

```shell
export HBASE_PID_DIR=/Users/huzekang/opt/hadoop-cdh/data/hbase/tmp/pids
```

ä½¿ç”¨å¤–éƒ¨zookeeper

```shell
export HBASE_MANAGES_ZK=false
```



#### hbase-site.xml

```xml
<configuration>
    <!-- é…ç½®æœ¬åœ°ä¸´æ—¶ç›®å½•ï¼Œç¼ºçœå€¼æ˜¯'/tmp' -->
    <property>
        <name>hbase.tmp.dir</name>
        <value>/Users/huzekang/opt/hadoop-cdh/data/hbase/tmp</value>
    </property>


    <!-- æŒ‡å®šå¤–éƒ¨zookeeperè¿æ¥ -->
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>localhost:2181</value>
    </property>


    <!-- æŒ‡å®šåˆ†å¸ƒå¼è¿è¡Œ -->
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>

    <!-- é…ç½®hbaseç›®å½•ï¼Œè®©HDFSç”Ÿæˆè¯¥ç›®å½•ç»™hbaseä½¿ç”¨ -->
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://localhost:8020/hbase</value>
    </property>
</configuration>
```



### å¯åŠ¨Hbase

```
~/opt/hadoop-cdh/hbase-1.2.0-cdh5.14.2 Â» bin/start-hbase.sh                                  
```

è§‚å¯Ÿå¯åŠ¨è¿›ç¨‹ï¼Œå…¶ä¸­hmasterå’Œhregionserverå°±æ˜¯hbaseçš„ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626145232.png)

æ‰“å¼€zookeeperå®¢æˆ·ç«¯

```
~/opt/hadoop-cdh/zookeeper-3.4.5-cdh5.14.2 Â» bin/zkCli.sh -server 127.0.0.1:2181
```

è§‚å¯Ÿ`/`ç›®å½•ä¸‹å¤šäº†ä¸€ä¸ªhbaseç›®å½•

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626144956.png)



### å…³é—­hbase

```
~/opt/hadoop-cdh/hbase-1.2.0-cdh5.14.2 Â» bin/stop-hbase.sh
```



### å¯åŠ¨hbase thrift

```
~/opt/hadoop-cdh/hbase-1.2.0-cdh5.14.2 Â» bin/hbase-daemon.sh start thrift
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627100355.png)

è¯¥è¿›ç¨‹å¯ä»¥æ–¹ä¾¿å…¶ä»–è¯­è¨€ä¾‹å¦‚pythonä½¿ç”¨hbaseçš„apiã€‚





## Hive

### å‚è€ƒæ–‡æ¡£ï¼š

å®˜æ–¹æ–‡æ¡£ï¼š<https://cwiki.apache.org/confluence/display/Hive/GettingStarted>

hiveserveræ­å»ºè¯¦è§£ï¼š<http://www.coderli.com/setup-hiveserver-step-details/>

### Metadata Store postgreså®‰è£…

Hiveæœ‰ä¸‰ç§å…ƒæ•°æ®å­˜å‚¨çš„æ¨¡å¼ã€‚

- Embedded mode
- Local mode
- Remote mode

å¾ˆå¤šæ–‡ç« é‡Œéƒ½æœ‰ä»‹ç»ï¼Œä½†æ˜¯å¾ˆå°‘æœ‰äººå…·ä½“è§£é‡Šå…¶åŒºåˆ«å’ŒåŸç†ã€‚

è¿™é‡Œæ‰¾åˆ°ä¸€ç¯‡ä»‹ç»ï¼Œè‡³å°‘å¯¹æˆ‘æ¥è¯´ï¼Œç®—æ˜¯èƒ½çœ‹æ‡‚ä»–æ‰€ä»‹ç»çš„ã€‚ [Metastore Deployment Modes](http://www.cloudera.com/documentation/enterprise/5-2-x/topics/cdh_ig_hive_metastore_configure.html#topic_18_4_1)

è¿™é‡Œé‡‡ç”¨**Remote mode**ã€‚å› æ­¤æˆ‘ä»¬å…ˆè¦éƒ¨ç½²**postgres**ã€‚



### é…ç½®Hive

**Hive**åœ¨**conf**ç›®å½•ä¸‹æä¾›äº†ä¸€ä¸ªé…ç½®æ–‡ä»¶æ¨¡ç‰ˆ**hive-default.xml.template**ä¾›æˆ‘ä»¬ä¿®æ”¹ã€‚å¦‚æœæ²¡ï¼Œåˆ™åˆ›å»ºä¸€ä»½ã€‚

**Hive**ä¼šé»˜è®¤åŠ è½½**hive-site.xml**é…ç½®æ–‡ä»¶ã€‚

```
cp conf/hive-default.xml.template conf/hive-site.xml
```

åŒæ ·ï¼ŒæŒ‰ç…§äº†è§£ï¼Œæˆ‘ä»¬éœ€è¦é…ç½®**metastore**çš„ç›¸å…³ä¿¡æ¯ï¼Œå¦‚æ•°æ®åº“è¿æ¥ï¼Œé©±åŠ¨ï¼Œç”¨æˆ·åï¼Œå¯†ç ç­‰ã€‚

#### é…ç½®hive-site.xmlé…ç½®æ–‡ä»¶

```xml
<?xml version="1.0"?>

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://192.168.5.148:5432/hive_metadata</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>postgres</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>eLN8QGV4g3LINDrFrsDKvCCyHapLOPCR</value>
    </property>

  <!-- ä½¿ç”¨è¿œç¨‹metastore -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://localhost:9083</value>
        <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
    </property>

  <!-- ä¿®å¤é—®é¢˜![](http://image-picgo.test.upcdn.net/img/20191215213150.png) -->
  <!-- hiveserver2å¼€æ”¾ç»™jdbcçš„ç«¯å£ -->
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10003</value>
  </property>

    <!-- è®¾ç½®hdfsä¸Šä¿å­˜çš„æ•°æ®ä»“åº“è·¯å¾„ -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/hive/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>




</configuration>


```

è¿™é‡Œï¼Œ**metastore**æœåŠ¡çš„é»˜è®¤ç«¯å£ä¸º**9083**ã€‚

ç”±äºæˆ‘ä»¬éœ€è¦è¿æ¥**pg10**æ•°æ®åº“ï¼Œè€Œ**Hive**å¹¶æ²¡æœ‰æä¾›ç›¸åº”çš„é©±åŠ¨ï¼Œæ‰€ä»¥éœ€è¦ä¸‹è½½**Pg  JDBC**é©±åŠ¨ï¼Œå¹¶æ”¾åœ¨**Hive**çš„**lib**ç›®å½•ä¸‹ã€‚ç›´æ¥è¿›å…¥**lib**ç›®å½•æ‰§è¡Œ

```
wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.1.4/postgresql-42.1.4.jar
```



#### åˆå§‹åŒ–å…ƒæ•°æ®çš„æ•°æ®åº“

1. åœ¨pgä¸­åˆ›å»ºæ•°æ®åº“`hive_metadata`

2. æ‰§è¡Œhiveè‡ªå¸¦çš„åˆå§‹åŒ–è„šæœ¬

   ```
   bin/schematool -dbType postgres -initSchema
   ```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626203349.png)



#### ä¿®æ”¹logæ—¥å¿—æ–‡ä»¶ä½ç½®(option)

**Hive**é»˜è®¤å°†æ—¥å¿—**å­˜æ”¾åœ¨/tmp/${user.name}**ä¸‹ã€‚ä¸ºäº†æ–¹ä¾¿ç»´æŠ¤å’ŒæŸ¥çœ‹ï¼Œä¿®æ”¹æ—¥å¿—æ–‡ä»¶ä½ç½®ã€‚

```
cp conf/hive-log4j.properties.template conf/hive-log4j.properties
```

ä¿®æ”¹æ—¥å¿—çº§åˆ«å’Œè¾“å‡ºæ—¥å¿—æ–‡ä»¶çš„åœ°å€ã€‚

```
hive.log.dir=/Users/huzekang/opt/hadoop-cdh/hive-1.1.0-cdh5.14.2/logs/hive
hive.root.logger=INFO,DRFA
```



### å¯åŠ¨MetaStoreServeræœåŠ¡

- å¯åŠ¨MetaStore Service

```
bin/hive --service metastore -p 9083 &
```

ç¬¬ä¸€æ¬¡å¯åŠ¨ä¼šåœ¨mysqlä¸­åˆ›å»ºè¡¨ï¼Œå¯èƒ½é€Ÿåº¦ä¼šæ…¢ç‚¹ã€‚

å¦‚ä¸Šå¯åŠ¨ï¼Œä¼šå¯åŠ¨ç«¯å£å·é»˜è®¤9083çš„metastoreæœåŠ¡ï¼Œä¹Ÿå¯ä»¥é€šè¿‡-pæŒ‡å®šç«¯å£å·ã€‚

å¯ä»¥é€šè¿‡çœ‹æ—¥å¿—æ–‡ä»¶è§‚å¯Ÿæ˜¯å¦æŠ¥é”™ã€‚

- ç›´æ¥å¯åŠ¨cliå³å¯

```
$HIVE_HOME/bin/hive
```



### å¯åŠ¨HiveServeræœåŠ¡ï¼Œä½¿å…¶ä»–æœåŠ¡å¯ä»¥é€šè¿‡thriftæ¥å…¥hive

å¯åŠ¨HiveServer

```
bin/hive --service hiveserver2 &
```

æŸ¥çœ‹æ—¥å¿—ä¸€åˆ‡æ­£å¸¸ã€‚å¯ä»¥çœ‹åˆ°å¤šäº†ä¸¤ä¸ªjavaè¿›ç¨‹ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623160521.png)

å¯ä»¥æ‰“å¼€æµè§ˆå™¨ç½‘å€<http://localhost:10002/hiveserver2.jsp>è¿›è¡Œè®¿é—®hiveserver2ã€‚

å¯åŠ¨æˆåŠŸåï¼Œhiveä¼šåœ¨hdfsä¸Šçš„tmpç›®å½•å»ºç«‹ç›¸å…³æ–‡ä»¶ï¼Œä½†æ˜¯ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€hdfsçš„tmpç›®å½•ä¼šå‘ç°æƒé™ä¸è¶³æ— æ³•è®¿é—®ã€‚ä¿®æ”¹tmpç›®å½•æƒé™å°±å¯ä»¥äº†ã€‚

```
bin/hdfs dfs -chmod -R 755 /tmp
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623163014.png)



### å¯åŠ¨æŠ¥é”™

![](http://image-picgo.test.upcdn.net/img/20191215213150.png)

10000æ˜¯hiveserver2å¼€æ”¾ç»™jdbcæœåŠ¡çš„ç«¯å£ï¼Œè¿™é‡Œè¢«å ç”¨äº†ã€‚

åŠ å…¥é…ç½®å³å¯

```xml
  <!-- hiveserver2å¼€æ”¾ç»™jdbcçš„ç«¯å£ -->
<property>
       <name>hive.server2.thrift.port</name>
      <value>10003</value>
 </property>
```





### ä½¿ç”¨hiveå®¢æˆ·ç«¯æµ‹è¯•

æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªtable

```sql
create table helloword(id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623155043.png)



æ’å…¥ä¸€æ¡æ•°æ®ã€‚

```sql
insert into helloword values(11,'peter');
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623161213.png)

æ‰“å¼€hdfså¯ä»¥è§‚å¯Ÿåˆ°æ•°æ®å­˜æ”¾åˆ°æŒ‡å®šçš„ç›®å½•äº†ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623161342.png)

å¯ä»¥åœ¨yarnä¸Šçœ‹åˆ°mrã€‚

![](http://image-picgo.test.upcdn.net/img/20191216003757.png)



### ä½¿ç”¨beelineè¿æ¥

####  åµŒå…¥æ¨¡å¼

```
bin/beeline -u jdbc:hive2://
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623220356.png)

æ­¤æ¨¡å¼å¯ä»¥æŸ¥è¯¢ï¼Œä¹Ÿå¯ä»¥æ’å…¥ã€‚



####    è¿œç¨‹æ¨¡å¼

```
bin/beeline      

beeline> !connect jdbc:hive2://localhost:10000
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626172239.png)

æ­¤æ¨¡å¼ä¸‹å¯ä»¥æ­£å¸¸æŸ¥è¯¢selectæ“ä½œï¼Œä½†æ˜¯æ’å…¥æ“ä½œå’Œåˆ›å»ºè¡¨éƒ½ä¼šæŠ¥é”™ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190623220827.png)

é”™è¯¯ä¿¡æ¯ï¼š

```java
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=anonymous, access=WRITE, inode="/Users/huzekang/opt/hadoop-cdh/hive-1.1.0-cdh5.14.2/warehouse":huzekang:supergroup:drwxr-xr-x
```

æ­¤æ—¶åªè¦å¯¹é”™è¯¯ä¿¡æ¯ä¸­æåˆ°çš„**hiveåœ¨hdfsä¸Šçš„warehouseç›®å½•**è¿›è¡Œchange the permissionå³å¯ã€‚

```
bin/hdfs dfs -chmod -R 777 /Users/huzekang/
```



### ä½¿ç”¨Javaä»£ç è¿æ¥

#### å¼•å…¥mavenä¾èµ–

```
 <dependency>
           <groupId>org.apache.hive</groupId>
           <artifactId>hive-jdbc</artifactId>
           <version>0.11.0</version>
       </dependency>
```

#### ä»£ç 

```java
/**
 * å¦‚æœé¢‘ç¹å‡ºç°ä¸‹é¢é”™è¯¯ï¼Œè¯•è¯•æ›´æ¢hiveç›®å½•ä¸‹çš„libç›®å½•ä¸‹çš„mysqlé©±åŠ¨
 * Error while compiling statement: FAILED: SemanticException Unable to fetch table hive_table1. Could not retrieve transation read-only status server
 */

import java.sql.*;

public class HiveJdbcTest {

    private static String driverName = "org.apache.hive.jdbc.HiveDriver";
   
    public static void main(String[] args) throws SQLException {
        try {
            Class.forName(driverName);
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
            System.exit(1);
        }
        // è®¾ç½®hive çš„jdbcè¿æ¥
        Connection con = DriverManager.getConnection("jdbc:hive2://localhost:10000/default", "", "");
        Statement stmt = con.createStatement();
        String tableName = "hive_table1";
        stmt.execute("drop table if exists " + tableName);
        stmt.execute("create table " + tableName +  " (key int, value string)");
        System.out.println("Create table success!");
        // show tables
        String sql = "show tables '" + tableName + "'";
        System.out.println("=======Running: " + sql);
        ResultSet res = stmt.executeQuery(sql);
        if (res.next()) {
            System.out.println(res.getString(1));
        }
 
        // describe table
        sql = "describe " + tableName;
        System.out.println("=======Running: " + sql);
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println("è¡¨å­—æ®µåï¼š"+res.getString(1) + "\t" +"è¡¨å­—æ®µç±»å‹ï¼š"+ res.getString(2));
        }
 

        sql = "select * from " + tableName;
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println(res.getInt(1) + "\t" + res.getString(2));
        }

        // æ’æ•°æ®åˆ°hive_table1
        sql = "insert into  " + tableName + " values (22,'xxded')";
        System.out.println("=======Running: " + sql);
        stmt.executeUpdate( sql);

        // zh
        sql = "select count(1) from " + tableName;
        System.out.println("Running: " + sql);
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println(res.getString(1));
        }

    }
}
```



### å¼€å¯updateå’Œdeleteæ“ä½œ

é»˜è®¤åœ¨hiveä¸­æ²¡æœ‰é»˜è®¤å¼€å¯æ”¯æŒå•æ¡æ’å…¥ï¼ˆupdateï¼‰ã€æ›´æ–°ä»¥åŠåˆ é™¤ï¼ˆdeleteï¼‰æ“ä½œï¼Œéœ€è¦è‡ªå·±é…ç½®ã€‚è€Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œå½“ç”¨æˆ·å¦‚æœä½¿ç”¨updateå’Œdeleteæ“ä½œæ—¶ï¼Œä¼šå‡ºç°å¦‚ä¸‹æƒ…å†µï¼š

```shell
hive> update dp set name='beijing' where id=1159;
FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.
```

**éœ€è¦åœ¨hive-site.xmlä¸­åŠ å…¥é…ç½®**

```xml
<!-- åŠ å…¥ä¸‹é¢é…ç½®å¯ä»¥è¿›è¡Œupdateå’Œdeleteæ“ä½œ -->
  <property>
<name>hive.support.concurrency</name>
    <value>true</value>
  </property>

    <property>
    <name>hive.enforce.bucketing</name>
    <value>true</value>
      </property>

  <property>
    <name>hive.exec.dynamic.partition.mode</name>
    <value>nonstrict</value>
  </property>

    <property>
    <name>hive.txn.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
  </property>

  <property>
    <name>hive.compactor.initiator.on</name>
    <value>true</value>
  </property>

    <property>
    <name>hive.compactor.worker.threads</name>
    <value>1</value>
  </property>

```



å†æ¬¡æ‰§è¡Œupdateå’Œdeleteæ“ä½œï¼Œä¾ç„¶æŠ¥é”™ã€‚

```shell
hive> update ods_user set sex = 1 where id = 10000000;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table hf_ods.ods_user that does not use an AcidOutputFormat or is not bucketed
hive> delete from ods_user  where id = 10000000;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table hf_ods.ods_user that does not use an AcidOutputFormat or is not bucketed

```



å¦‚æœä¸€ä¸ªè¡¨è¦å®ç°updateå’ŒdeleteåŠŸèƒ½ï¼Œè¯¥è¡¨å°±å¿…é¡»æ”¯æŒACIDï¼Œè€Œæ”¯æŒACIDï¼Œå°±å¿…é¡»æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

- è¡¨çš„å­˜å‚¨æ ¼å¼å¿…é¡»æ˜¯ORCï¼ˆSTORED AS ORCï¼‰ï¼›

- è¡¨å¿…é¡»è¿›è¡Œåˆ†æ¡¶ï¼ˆCLUSTERED BY (col_name, col_name, â€¦) INTO num_buckets BUCKETSï¼‰ï¼›

- Table propertyä¸­å‚æ•°transactionalå¿…é¡»è®¾å®šä¸ºTrueï¼ˆtblproperties(â€˜transactionalâ€™=â€˜trueâ€™)ï¼‰ï¼›

  ä¿®æ”¹å»ºè¡¨è¯­å¥:

  ```sql
  create table if not exists tablename (
    id bigint,
    age bigint  COMMENT 'å¹´é¾„',
    name String COMMENT 'å§“å'
  ) COMMENT 'ç”¨æˆ·è¡¨'
  partitioned by (year string)
  clustered by (id) into 2 buckets
  row format delimited fields terminated by '\t'
  stored as orc TBLPROPERTIES('transactional'='true');
  ```

  

### Spark standaloneé›†ç¾¤æ¨¡å¼éƒ¨ç½²

1. å°†confç›®å½•ä¸‹çš„`slaves.template`æ–‡ä»¶å¤åˆ¶æˆ`slaves`ï¼Œå¹¶å†™å…¥workeréƒ¨ç½²çš„ä¸»æœº![](https://i.loli.net/2019/09/30/qNhwvczkUrZO45e.png)

2. å°†sparkç›®å½•åˆ†å‘åˆ°å…¶ä»–ä¸»æœº

   ```
   scp -r /opt/spark cdh02:/opt
   ```

3. åœ¨ä¸»èŠ‚ç‚¹çš„sparkç›®å½•ä¸‹å¯åŠ¨é›†ç¾¤

   ```
   sbin/start-all.sh
   ```

   

### Sparké›†ç¾¤å¯åŠ¨å‘½ä»¤æ±‡æ€»

1ã€åœ¨ä¸»èŠ‚ç‚¹å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ˆåŒ…æ‹¬slaveèŠ‚ç‚¹ï¼Œéœ€è¦åšå…å¯†ç ç™»å½•ï¼‰

```
sbin/start-all.sh
```


2ã€å•ç‹¬å¯åŠ¨ä¸»èŠ‚ç‚¹

```
sbin/start-master.sh
```

3ã€å•ç‹¬å¯åŠ¨slaveèŠ‚ç‚¹

å¯åŠ¨æ‰€æœ‰çš„slavesèŠ‚ç‚¹

```
sbin/start-slaves.sh spark://10.130.2.220:7077
```

å¯åŠ¨å•å°çš„slavesèŠ‚ç‚¹

```
sbin/start-slave.sh spark://10.130.2.220:7077
```

å¯åŠ¨åå¯ä»¥æ‰“å¼€æµè§ˆå™¨`http://localhost:8080/`çœ‹åˆ°spark masterã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627113457.png)

å¯ä»¥è§‚å¯Ÿåˆ°èµ·æ¥äº†ä¸€ä¸ªmasterå’Œworkerè¿›ç¨‹ã€‚

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190626112610.png)



### Livy spark

ç”¨äºsparkçš„äº¤äº’å¼ç¼–ç¨‹å’Œæäº¤æ‰¹å¤„ç†ä½œä¸šåˆ°saprké›†ç¾¤

1. ä¸‹è½½

   ```
   wget http://mirrors.tuna.tsinghua.edu.cn/apache/incubator/livy/0.6.0-incubating/apache-livy-0.6.0-incubating-bin.zip
   ```

2. ä¿®æ”¹confç›®å½•ä¸‹çš„`livy-env.sh`æ–‡ä»¶

   ```
   export SPARK_HOME=/opt/spark-2.4.4-bin-hadoop2.6
   export HADOOP_CONF_DIR=/etc/hadoop/conf
   ```

3. ä¿®æ”¹confç›®å½•ä¸‹çš„`livy.conf`æ–‡ä»¶

   ```
   # What spark master Livy sessions should use.
    livy.spark.master = spark://cdh01:7077
   
   # What spark deploy mode Livy sessions should use.
    livy.spark.deploy-mode = cluster
   ```

4. ä½¿ç”¨postmanå¼•å…¥æµ‹è¯•api

   https://www.getpostman.com/collections/ee87d500b01a06343d03







## Presto

### æœåŠ¡ç«¯å®‰è£…

#### 1. ä¸‹è½½

#### 2. é…ç½®

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627204309.png)

1. `etc/config.properties`

   ```properties
   coordinator=true
   node-scheduler.include-coordinator=true
   http-server.http.port=8082
   query.max-memory=5GB
   query.max-memory-per-node=1GB
   query.max-total-memory-per-node=2GB
   discovery-server.enabled=true
   discovery.uri=http://localhost:8082
   ```

2. `etc/jvm.config`

   ```java
   -server
   -Xmx16G
   -XX:+UseG1GC
   -XX:G1HeapRegionSize=32M
   -XX:+UseGCOverheadLimit
   -XX:+ExplicitGCInvokesConcurrent
   -XX:+HeapDumpOnOutOfMemoryError
   -XX:+ExitOnOutOfMemoryError
   ```

3. `etc/node.properties`

   ```properties
   node.environment=production
   node.id=ffffffff-ffff-ffff-ffff-ffffffffffff
   node.data-dir=/Users/huzekang/opt/hadoop-cdh/data/presto
   ```

#### 3. å¯åŠ¨

daemonè¿è¡Œï¼š`bin/launcher start` 
foregroundè¿è¡Œï¼š`bin/launcher run`

```
~/opt/hadoop-cdh/presto-server-0.221 Â» bin/launcher run
```



### å®¢æˆ·ç«¯å®‰è£…

1. ä¸‹è½½https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.196/presto-cli-0.196-executable.jar

2. ç§»åŠ¨åˆ°ä½ å–œæ¬¢çš„ç›®å½•

   ```shell
   ~/opt/hadoop-cdh/presto-server-0.221/bin Â» mv ~/Downloads/presto-cli-0.196-executable.jar ./presto            
   ```

3. èµ‹äºˆå®ƒæ‰§è¡Œçš„æƒé™

   ```
    chmod +x presto
   ```



### é…ç½®è¿æ¥å™¨

#### mysql

å‚è€ƒé…ç½®<https://prestodb.github.io/docs/current/connector/mysql.html>

åœ¨serverçš„ä¸»ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶`etc/catalog/mysql.properties`

```properties
connector.name=mysql
connection-url=jdbc:mysql://192.168.1.150:3306
connection-user=root
connection-password=root
```

é‡å¯å®Œserveråå¯åŠ¨å®¢æˆ·ç«¯

```
~/opt/hadoop-cdh/presto-server-0.221/bin Â» ./presto --server localhost:8082 --catalog mysql --schema yiboard  
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627165459.png)



#### postgres

å‚è€ƒé…ç½®ï¼š<https://prestodb.github.io/docs/current/connector/postgresql.html>

åœ¨serverçš„ä¸»ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶`etc/catalog/posgresql.properties`

```properties
connector.name=postgresql
connection-url=jdbc:postgresql://192.168.1.150:5432/crawl
connection-user=postgres
connection-password=123456
```

é‡å¯å®Œserveråå¯åŠ¨å®¢æˆ·ç«¯ã€‚

```
~/opt/hadoop-cdh/presto-server-0.221/bin Â» ./presto --server localhost:8082 --catalog postgresql --schema public
```



#### mongoDB

å‚è€ƒé…ç½®ï¼š<https://prestodb.github.io/docs/current/connector/mongodb.html>>

åœ¨serverçš„ä¸»ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶`etc/catalog/mongodb.properties`

```properties
connector.name=mongodb
mongodb.seeds=192.168.1.150:27017
```

é‡å¯å®Œserveråå¯åŠ¨å®¢æˆ·ç«¯ã€‚

```
~/opt/hadoop-cdh/presto-server-0.221/bin Â» ./presto --server localhost:8082 --catalog mongodb --schema yibo
```



#### hive

å‚è€ƒé…ç½®ï¼š<https://prestodb.github.io/docs/current/connector/hive.html>>

åœ¨serverçš„ä¸»ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶`etc/catalog/posgresql.properties`

```properties
connector.name=hive-hadoop2
hive.metastore.uri=thrift://localhost:9083
```

é‡å¯å®Œserveråå¯åŠ¨å®¢æˆ·ç«¯ã€‚

```
~/opt/hadoop-cdh/presto-server-0.221/bin Â» ./presto --server localhost:8082 --catalog hive --schema default
```



### è·¨æºæŸ¥è¯¢

è¿™é‡Œæˆ‘ä½¿ç”¨mysqlçš„è¡¨å’Œpgçš„è¡¨è¿›è¡Œå…³è”æŸ¥è¯¢ã€‚

```sql
SELECT a.employee_id,b.id, a.full_name,b.repo_name FROM mysql.foodmart.employee a,postgresql.public.github_repo b where a.employee_id = b.id;
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190628141632.png)



## Kafka

#### ä¸‹è½½è§£å‹

#### é…ç½®

é…ç½®`config/server.properties`æ–‡ä»¶ã€‚

```
broker.id=1
log.dirs=/Users/huzekang/opt/hadoop-cdh/data/kafka/kafka-logs
```



#### å¯åŠ¨

é¦–å…ˆæ‰“å¼€zookeeperã€‚

ç„¶åå¯åŠ¨kafka serverã€‚

```
~/opt/hadoop-cdh/kafka_2.12-2.1.1 Â» bin/kafka-server-start.sh  config/server.properties
```

![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627230614.png)



#### æµ‹è¯•

1. å¯åŠ¨æ§åˆ¶å°producerï¼Œåˆ›å»ºä¸€ä¸ªtest2çš„topicï¼Œè¾“å‡ºå‡ ä¸ªhello

   ```shell
   ~/opt/hadoop-cdh/kafka_2.12-2.1.1 Â»  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test2         
   >hello1
   [2019-06-27 23:08:54,369] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : {test2=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
   >hello2
   >hello3
   >
   ```

2. å¯åŠ¨æ§åˆ¶å°consumerï¼Œç›‘å¬test2çš„topicã€‚

   ```
   bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test2 --from-beginning
   ```

   å¯ä»¥çœ‹åˆ°ğŸ‘†ä¸Šé¢è¾“å…¥çš„éƒ½æ˜¾ç¤ºå‡ºæ¥äº†ã€‚

   ![](https://raw.githubusercontent.com/huzekang/picbed/master/20190627231059.png)

3. **æŸ¥çœ‹æœ‰å“ªäº›topic**

4. ```
   bin/kafka-topics.sh --list --zookeeper localhost:2181
   ```

   ![](https://raw.githubusercontent.com/huzekang/picbed/master/20190701213535.png)